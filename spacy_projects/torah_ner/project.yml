title: "Linker Pipeline"
description: "All preprocessing and training for linker models"
spacy_version: ">=3.2.0,<4.0.0"
# Variables can be referenced across the project.yml using ${vars.var_name}
vars:
  name: "ner"
  lang: "he"
  training_input: ""
  config: ""
  annotator_pod: "annotator-bodum-0"
  webpages_dir: "../web_scraper/output"
  embedding_size: 50
  pretrain_model_name: "model8.bin"
  # number of processes (tokenization) and threads (fasttext)
  n_process: 8
  min_training_text_len: 20
  training_percentage: 0.8
  random_seed: 61  # determined to be sufficiently random
  training_input_type: ""
env:
  mongo_host: MONGO_HOST
  mongo_port: MONGO_PORT
  user: MONGO_USER
  password: MONGO_PASSWORD
  replicaset_name: REPLICASET_NAME
  gpu_id: GPU_ID


# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories: ["corpus", "vectors", "scripts", "configs", "models"]

## Workflows are sequences of commands (see below) executed in order. You can
## run them via "spacy project run [workflow]". If a commands's inputs/outputs
## haven't changed, it won't be re-run.
workflows:
  all:
    - download-sefaria-dump
    - export-library
    - train-fasttext
    - pretrain
    - train-ner

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "spacy project run [command] [path]". The help message is optional and
# shown when executing "spacy project run [optional command] [path] --help".
commands:
  - name: "download-sefaria-dump"  # this command is irrelevant for usage in the ML job (util/job.py) as it is only intended for usage when Sefaria-Project is set up
    script:
      - "sh ../util/restore_db.sh"
    deps:
      # actually dependent on dump file in Google Cloud...
      - "../util/restore_db.sh"

  - name: "export-library"  # this is dependent on Sefaria-Project and so doesn't work in the ML job (util/job.py)
    script:
      - "python ../util/library_exporter.py ${vars.lang} -f both -w ${vars.webpages_dir} -o corpus/all_text_${vars.lang} -s"
    deps:
      - "../util/restore_db.sh"
    outputs:
      - "corpus/all_text_${vars.lang}.txt"
      - "corpus/all_text_${vars.lang}.jsonl"

  - name: export-training-data-from-mongo
    script:
      - "python ../util/merge_collections.py -o merged_output -m ${env.mongo_host} -p ${env.mongo_port} -c ${vars.training_input} --user ${env.user} --replicaset ${env.replicaset_name} "
      - "python ../util/convert_training_data_to_docbin.py ${vars.lang} merged_output corpus/${vars.name}_${vars.lang} ${vars.min_training_text_len} ${vars.training_percentage} ${vars.random_seed} --input-type mongo --db-host ${env.mongo_host} --db-port ${env.mongo_port} --user ${env.user}  --replicaset ${env.replicaset_name}"

  - name: export-training-data-from-json
    script:
      - "python ../util/convert_training_data_to_docbin.py ${vars.lang} ${vars.training_input} corpus/${vars.name}_${vars.lang} ${vars.min_training_text_len} ${vars.training_percentage} ${vars.random_seed} --input-type json"

  - name: train-fasttext
    script:
      - "python ../util/embedding_scripts.py fasttext -d ${vars.embedding_size} -i corpus/all_text_${vars.lang}.txt -o vectors/all_text_${vars.lang}.fasttext_${vars.embedding_size}"
      - "python -m spacy init vectors ${vars.lang} vectors/all_text_${vars.lang}.fasttext_${vars.embedding_size}.vec vectors/all_text_${vars.lang}_fasttext_model_${vars.embedding_size}"
    deps:
      - "../util/embedding_scripts.py"
      - "corpus/all_text_${vars.lang}.txt"
    outputs:
      - vectors/all_text_${vars.lang}.fasttext_${vars.embedding_size}.bin
      - vectors/all_text_${vars.lang}.fasttext_${vars.embedding_size}.vec
      - "vectors/all_text_${vars.lang}_fasttext_model_${vars.embedding_size}"

  - name: "pretrain"
    script:
      - "python -m spacy pretrain ${vars.config} models/pretrain_ref_${vars.lang}_${vars.embedding_size} --paths.vectors vectors/all_text_${vars.lang}_fasttext_model_${vars.embedding_size} --paths.raw_text corpus/all_text_${vars.lang}.jsonl --nlp.lang ${vars.lang} --pretraining.objective.hidden_size ${vars.embedding_size} --code ../../util/spacy_registry.py --gpu-id ${env.gpu_id}"
    deps:
      - "${vars.config}"
      - corpus/all_text_${vars.lang}.jsonl
      - vectors/all_text_${vars.lang}_fasttext_model_${vars.embedding_size}
    outputs:
      - "models/pretrain_ref_${vars.lang}_${vars.embedding_size}"

  - name: "train-ner"
    script:
      - "python -m spacy train ${vars.config} --output models/${vars.name}_${vars.lang} --paths.vectors vectors/all_text_${vars.lang}_fasttext_model_${vars.embedding_size} --paths.train corpus/${vars.name}_${vars.lang}_train.spacy --paths.dev corpus/${vars.name}_${vars.lang}_test.spacy --nlp.lang ${vars.lang} --paths.init_tok2vec models/pretrain_ref_${vars.lang}_${vars.embedding_size}/${vars.pretrain_model_name} --pretraining.objective.hidden_size ${vars.embedding_size} --code ../../util/spacy_registry.py --gpu-id ${env.gpu_id}"

    deps:
      - "${vars.config}"
      - "models/pretrain_ref_${vars.lang}_${vars.embedding_size}"
      - "corpus/${vars.name}_${vars.lang}_train.spacy"
      - "corpus/${vars.name}_${vars.lang}_test.spacy"
      - "vectors/all_text_${vars.lang}_fasttext_model_${vars.embedding_size}"
    outputs:
      - "models/${vars.name}_${vars.lang}"

  - name: "train-blank-pretrained"
    script:
      - "python -m spacy train ${vars.config} --output models/pretrain_usable --paths.vectors vectors/all_text_${vars.lang}_fasttext_model_${vars.embedding_size} --paths.train corpus/${vars.name}_${vars.lang}_train.spacy --paths.dev corpus/${vars.name}_${vars.lang}_test.spacy --nlp.lang ${vars.lang} --paths.init_tok2vec models/pretrain_ref_${vars.lang}_${vars.embedding_size}/${vars.pretrain_model_name} --training.max_steps 1 --pretraining.objective.hidden_size ${vars.embedding_size} --code ../../util/spacy_registry.py --gpu-id ${env.gpu_id}"
    deps:
      - "${vars.config}"
      - "models/pretrain_ref_${vars.lang}_${vars.embedding_size}"
    outputs:
      - "models/pretrain_usable"
