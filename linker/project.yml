title: "Linker Pipeline"
description: "All preprocessing and training for linker models"
spacy_version: ">=3.2.0,<4.0.0"
# Variables can be referenced across the project.yml using ${vars.var_name}
vars:
  name: "webpages"
  lang: "he"
  # ref_training_collections: "achronim_output webpages_output rishonim_output_silver gilyon_output"
  ref_training_collections: "webpages_output achronim_output gilyon_output:100"
  subref_training_collections: "webpages_sub_citation_output gilyon_sub_citation_output"
  annotator_pod: "annotator-bodum-0"
  # number of processes (tokenization) and threads (fasttext)
  n_process: 8


# These are the directories that the project needs. The project CLI will make
# sure that they always exist.
directories: ["corpus", "vectors", "scripts", "configs", "models"]

## Workflows are sequences of commands (see below) executed in order. You can
## run them via "spacy project run [workflow]". If a commands's inputs/outputs
## haven't changed, it won't be re-run.
workflows:
  all:
    #- download-sefaria-dump
    #- export-library
    #- train-floret
    #- init-floret-vectors
    - train-fasttext
    - init-fasttext-vectors
    - pretrain
    #- train-ref-model

# Project commands, specified in a style similar to CI config files (e.g. Azure
# pipelines). The name is the command name that lets you trigger the command
# via "spacy project run [command] [path]". The help message is optional and
# shown when executing "spacy project run [optional command] [path] --help".
commands:
  - name: "download-sefaria-dump"
    script:
      - "sh scripts/restore_db.sh"
    deps:
      # actually dependent on dump file in Google Cloud...
      - "scripts/restore_db.sh"

  - name: "export-library"
    script:
      - "python scripts/library_exporter.py ${vars.lang} -f both -w corpus/all_text_webpages_he.txt -o corpus/all_text_${vars.lang}"
    deps:
      - "scripts/restore_db.sh"
    outputs:
      - "corpus/all_text_${vars.lang}.txt"
      - "corpus/all_text_${vars.lang}.jsonl"

  - name: "train-floret"
    help: "Train floret vectors"
    script:
      - "python scripts/train_floret.py --model cbow --dim 300 --mincount 10 --minn 3 --maxn 6 --neg 10 --mode floret --hashcount 2 --bucket 20000 --thread ${vars.n_process} --epoch 10 corpus/all_text_${vars.lang}.txt vectors/all_text_${vars.lang}"
    deps:
      - "scripts/train_floret.py"
      - "corpus/all_text_${vars.lang}.txt"
    outputs:
      - "vectors/all_text_${vars.lang}.floret"
      - "vectors/all_text_${vars.lang}.vec"
      - "vectors/all_text_${vars.lang}.bin"

  - name: "init-floret-vectors"
    help: "Create a floret vectors model"
    script:
      - "python -m spacy init vectors ${vars.lang} vectors/all_text_${vars.lang}.floret vectors/all_text_${vars.lang}_floret_model --mode floret"
    deps:
      - "vectors/all_text_${vars.lang}.floret"
    outputs:
      - "vectors/all_text_${vars.lang}_floret_model"

  - name: train-fasttext
    script:
      - "python scripts/embedding_scripts.py fasttext -d 300 -i corpus/all_text_${vars.lang}.txt -o vectors/all_text_${vars.lang}.fasttext"
    deps:
      - scripts/embedding_scripts.py
      - "corpus/all_text_${vars.lang}.txt"
    outputs:
      - vectors/all_text_${vars.lang}.fasttext.bin
      - vectors/all_text_${vars.lang}.fasttext.vec

  - name: "init-fasttext-vectors"
    script:
      - "python -m spacy init vectors ${vars.lang} vectors/all_text_${vars.lang}.fasttext.vec vectors/all_text_${vars.lang}_fasttext_model"
    deps:
      - vectors/all_text_${vars.lang}.fasttext.vec
    outputs:
      - "vectors/all_text_${vars.lang}_fasttext_model"

  - name: "pretrain"
    script:
      - "python -m spacy pretrain configs/ref-v3.2.cfg models/pretrain_ref_${vars.lang} --paths.vectors vectors/all_text_${vars.lang}_fasttext_model --paths.raw_text corpus/all_text_${vars.lang}.jsonl --paths.input_collection ${vars.name}_output --nlp.lang ${vars.lang} --code ../util/spacy_registry.py --gpu-id 0"
    deps:
      - "configs/ref-v3.2.cfg"
      - vectors/all_text_${vars.lang}_fasttext_model
    outputs:
      - "models/pretrain_ref_${vars.lang}"

  - name: "train-ref-model"
    script:
      - "python ../util/merge_collections.py -o merged_output -c ${vars.ref_training_collections}"
      - "python -m spacy train configs/ref-v3.2.cfg --output models/${vars.name}_${vars.lang}_achronim --paths.vectors vectors/all_text_${vars.lang}_fasttext_model --paths.input_collection merged_output --nlp.lang ${vars.lang} --paths.init_tok2vec models/pretrain_ref_${vars.lang}/model8.bin --code ../util/spacy_registry.py --gpu-id 0"
    deps:
      - "configs/ref-v3.2.cfg"
      - "models/pretrain_ref_${vars.lang}"
    outputs:
      - "models/${vars.name}_${vars.lang}"

  - name: "train-blank-pretrained-model"
    script:
      - "python ../util/merge_collections.py -o merged_output -c achronim_output:2"
      - "python -m spacy train configs/ref-v3.2.cfg --output models/pretrain_usable --paths.vectors vectors/all_text_${vars.lang}_fasttext_model --paths.input_collection merged_output --nlp.lang ${vars.lang} --paths.init_tok2vec models/pretrain_ref_${vars.lang}/model8.bin --training.max_steps 1 --code ../util/spacy_registry.py --gpu-id 0"
    deps:
      - "configs/ref-v3.2.cfg"
      - "models/pretrain_ref_${vars.lang}"
    outputs:
      - "models/pretrain_usable"

  - name: "train-subref-model"
    script:
      - "python ../util/merge_collections.py -o merged_subref_output -c ${vars.subref_training_collections}"
      - "python -m spacy train configs/subref-v3.2.cfg --output models/${vars.name}_subref_${vars.lang} --paths.vectors vectors/all_text_${vars.lang}_fasttext_model --paths.input_collection merged_subref_output --nlp.lang ${vars.lang} --paths.init_tok2vec models/pretrain_ref_${vars.lang}/model8.bin --code ../util/spacy_registry.py --gpu-id 0"
    deps:
      - "configs/subref-v3.2.cfg"
      - "models/pretrain_ref_${vars.lang}"
    outputs:
      - "models/${vars.name}_subref_${vars.lang}"

  - name: "upload-models"
    help: "package and upload ref and subref models to annotator cauldron"
    script:
      # ref model
      #- "tar cf models/${vars.name}_${vars.lang}.tar models/${vars.name}_${vars.lang}/model-best"
      #- "kubectl exec -it ${vars.annotator_pod} -- apt-get install -y rsync"  # make sure rsync exists on pod
      #- "krsync.sh -av --progress --stats models/${vars.name}_${vars.lang}.tar ${vars.annotator_pod}:/prodigy-disk/${vars.name}_${vars.lang}.tar"
      #- "kubectl exec -it ${vars.annotator_pod} -- tar xf /prodigy-disk/${vars.name}_${vars.lang}.tar"
      - "kubectl exec -it ${vars.annotator_pod} -- rm -rf /prodigy-disk/${vars.name}_${vars.lang}"
      - "kubectl exec -it ${vars.annotator_pod} -- mv models/${vars.name}_${vars.lang}/model-best /prodigy-disk/${vars.name}_${vars.lang}"
      - "kubectl exec -it ${vars.annotator_pod} -- rm -rf models /prodigy-disk/${vars.name}_${vars.lang}.tar"
      - "rm -rf models/${vars.name}_${vars.lang}.tar"
      # subref model
      - "tar cf models/${vars.name}_subref_${vars.lang}.tar models/${vars.name}_subref_${vars.lang}/model-best"
      - "krsync.sh -av --progress --stats models/${vars.name}_subref_${vars.lang}.tar ${vars.annotator_pod}:/prodigy-disk/${vars.name}_subref_${vars.lang}.tar"
      - "kubectl exec -it ${vars.annotator_pod} -- tar xf /prodigy-disk/${vars.name}_subref_${vars.lang}.tar"
      - "kubectl exec -it ${vars.annotator_pod} -- rm -rf /prodigy-disk/${vars.name}_subref_${vars.lang}"
      - "kubectl exec -it ${vars.annotator_pod} -- mv models/${vars.name}_subref_${vars.lang}/model-best /prodigy-disk/${vars.name}_subref_${vars.lang}"
      - "kubectl exec -it ${vars.annotator_pod} -- rm -rf models /prodigy-disk/${vars.name}_subref_${vars.lang}.tar"
      - "rm -rf models/${vars.name}_subref_${vars.lang}.tar"
